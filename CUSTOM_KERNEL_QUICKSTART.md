# è‡ªå®šä¹‰CUDA Kernelå¿«é€Ÿå¼€å§‹æŒ‡å—

æœ¬æŒ‡å—å¸®åŠ©ä½ å¿«é€Ÿä¸Šæ‰‹åœ¨nano-vllmæ¡†æ¶ä¸­ä½¿ç”¨å’Œå¼€å‘è‡ªå®šä¹‰CUDA kernelã€‚

## ğŸ“š æ–‡æ¡£ç»“æ„

1. **CUSTOM_CUDA_KERNEL_GUIDE.md** - å®Œæ•´çš„å®ç°æŒ‡å—ï¼ˆCUDA C++å’ŒTritonï¼‰
2. **æœ¬æ–‡æ¡£** - å¿«é€Ÿå¼€å§‹å’Œå¸¸è§ç”¨ä¾‹

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æµ‹è¯•ç°æœ‰çš„è‡ªå®šä¹‰Kernel

```bash
# è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶
python test_custom_kernels.py
```

è¿™å°†æµ‹è¯•ï¼š
- âœ“ Kernelæ­£ç¡®æ€§éªŒè¯
- âœ“ æ€§èƒ½benchmark
- âœ“ å†…å­˜ä½¿ç”¨åˆ†æ
- âœ“ Profileråˆ†æ

### 2. æµ‹è¯•è‡ªå®šä¹‰MLPå±‚

```bash
# æµ‹è¯•ä½¿ç”¨è‡ªå®šä¹‰kernelçš„MLPå±‚
python -m nanovllm.layers.custom_mlp
```

### 3. åœ¨Benchmarkä¸­ä½¿ç”¨Profiling

```bash
# è¿è¡Œå¸¦profilingçš„benchmark
python bench.py
```

è¾“å‡ºæ–‡ä»¶ï¼š
- `profile_trace.json` - Chrome traceæ–‡ä»¶ï¼ˆåœ¨chrome://tracingä¸­æŸ¥çœ‹ï¼‰
- `profile_report.txt` - æ–‡æœ¬æ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: ç›´æ¥ä½¿ç”¨Kernel

```python
import torch
from nanovllm.kernels import fused_add_gelu, element_wise_mul_add

# Fused Add + GELU
x = torch.randn(16, 128, 4096, device='cuda')
bias = torch.randn(4096, device='cuda')
output = fused_add_gelu(x, bias)

# Element-wise Mul + Add
a = torch.randn(16, 128, 4096, device='cuda')
b = torch.randn(16, 128, 4096, device='cuda')
c = torch.randn(16, 128, 4096, device='cuda')
result = element_wise_mul_add(a, b, c)  # a * b + c
```

### ç¤ºä¾‹2: åœ¨è‡ªå®šä¹‰å±‚ä¸­ä½¿ç”¨

```python
from torch import nn
from nanovllm.kernels import fused_add_gelu

class MyCustomLayer(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.linear = nn.Linear(hidden_size, hidden_size)
        self.bias = nn.Parameter(torch.zeros(hidden_size))
    
    def forward(self, x):
        x = self.linear(x)
        # ä½¿ç”¨è‡ªå®šä¹‰kernelè€Œä¸æ˜¯ F.gelu(x + bias)
        return fused_add_gelu(x, self.bias)
```

### ç¤ºä¾‹3: é›†æˆåˆ°ç°æœ‰æ¨¡å‹

åœ¨`nanovllm/models/qwen3.py`ä¸­ä½¿ç”¨è‡ªå®šä¹‰MLPï¼š

```python
from nanovllm.layers.custom_mlp import CustomGatedMLP

class Qwen3DecoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.self_attn = Qwen3Attention(config)
        
        # æ›¿æ¢æ ‡å‡†MLPä¸ºè‡ªå®šä¹‰ä¼˜åŒ–ç‰ˆæœ¬
        self.mlp = CustomGatedMLP(
            hidden_size=config.hidden_size,
            intermediate_size=config.intermediate_size,
            use_custom_kernels=True,
        )
        # ...
```

## ğŸ”§ å¼€å‘æ–°çš„Kernel

### æ–¹æ³•1: ä½¿ç”¨Tritonï¼ˆæ¨èï¼‰

```python
import triton
import triton.language as tl

@triton.jit
def my_custom_kernel(
    input_ptr,
    output_ptr,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    
    # åŠ è½½æ•°æ®
    x = tl.load(input_ptr + offsets, mask=mask)
    
    # ä½ çš„è®¡ç®—é€»è¾‘
    output = x * 2.0  # ç¤ºä¾‹
    
    # å­˜å‚¨ç»“æœ
    tl.store(output_ptr + offsets, output, mask=mask)

# Pythonæ¥å£
def my_custom_op(x):
    output = torch.empty_like(x)
    n_elements = x.numel()
    BLOCK_SIZE = 1024
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
    
    my_custom_kernel[grid](x, output, n_elements, BLOCK_SIZE=BLOCK_SIZE)
    return output
```

### æ–¹æ³•2: ä½¿ç”¨CUDA C++ï¼ˆæœ€é«˜æ€§èƒ½ï¼‰

å‚è€ƒ`CUSTOM_CUDA_KERNEL_GUIDE.md`ä¸­çš„è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. Benchmarkä½ çš„Kernel

```python
from nanovllm.kernels import benchmark_kernel

# å¯¹æ¯”è‡ªå®šä¹‰kernelå’ŒPyTorchå®ç°
time_custom = benchmark_kernel(my_custom_op, x, warmup=20, iters=100)
time_torch = benchmark_kernel(torch_reference, x, warmup=20, iters=100)

print(f"åŠ é€Ÿæ¯”: {time_torch/time_custom:.2f}x")
```

### 2. ä½¿ç”¨Profiler

```python
from torch.profiler import profile, ProfilerActivity

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
) as prof:
    output = my_custom_op(x)

print(prof.key_averages().table(sort_by="cuda_time_total"))
prof.export_chrome_trace("trace.json")
```

### 3. ä¼˜åŒ–å»ºè®®

- **Kernelèåˆ**: åˆå¹¶å¤šä¸ªæ“ä½œå‡å°‘å†…å­˜è®¿é—®
- **å†…å­˜å¸ƒå±€**: ç¡®ä¿tensoræ˜¯è¿ç»­çš„ï¼ˆ`contiguous()`ï¼‰
- **Blockå¤§å°**: ä½¿ç”¨`triton.next_power_of_2()`è‡ªåŠ¨è°ƒä¼˜
- **æ•°å€¼ç²¾åº¦**: æ ¹æ®éœ€æ±‚ä½¿ç”¨FP16/BF16

## ğŸ¯ å¸¸è§ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: èåˆç®—å­åŠ é€Ÿ

**é—®é¢˜**: å¤šä¸ªå°ç®—å­é¢‘ç¹è®¿é—®å†…å­˜
**è§£å†³**: ä½¿ç”¨fused kernel

```python
# æ…¢: ä¸¤æ¬¡å†…å­˜è®¿é—®
x = x + bias
x = F.gelu(x)

# å¿«: ä¸€æ¬¡å†…å­˜è®¿é—®
x = fused_add_gelu(x, bias)
```

### åœºæ™¯2: è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°

```python
@triton.jit
def custom_activation_kernel(...):
    # å®ç°è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°
    # ä¾‹å¦‚: Swish, Mish, æˆ–å…¶ä»–
    pass
```

### åœºæ™¯3: ä¼˜åŒ–Attentionè®¡ç®—

æ¡†æ¶å·²ç»åœ¨`nanovllm/layers/attention.py`ä¸­ä½¿ç”¨Tritonä¼˜åŒ–KV cacheå­˜å‚¨ã€‚

## ğŸ› è°ƒè¯•æŠ€å·§

### 1. éªŒè¯æ­£ç¡®æ€§

```python
# å¯¹æ¯”è‡ªå®šä¹‰kernelå’Œå‚è€ƒå®ç°
output_custom = my_custom_op(x)
output_reference = reference_implementation(x)

assert torch.allclose(output_custom, output_reference, rtol=1e-5)
```

### 2. æ‰“å°ä¸­é—´ç»“æœ

```python
print(f"Input: shape={x.shape}, dtype={x.dtype}, device={x.device}")
print(f"Output: shape={output.shape}")
print(f"Max diff: {(output_custom - output_reference).abs().max()}")
```

### 3. æ£€æŸ¥å†…å­˜

```python
torch.cuda.reset_peak_memory_stats()
output = my_custom_op(x)
mem = torch.cuda.max_memory_allocated() / 1024**2
print(f"Peak memory: {mem:.2f} MB")
```

## ğŸ“ˆ é›†æˆåˆ°ç”Ÿäº§ç¯å¢ƒ

### æ­¥éª¤1: å……åˆ†æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python test_custom_kernels.py

# Benchmarkä¸åŒé…ç½®
python bench.py
```

### æ­¥éª¤2: æ·»åŠ Fallback

```python
def safe_custom_op(x):
    if x.is_cuda and KERNELS_AVAILABLE:
        return custom_kernel(x)
    else:
        return torch_fallback(x)
```

### æ­¥éª¤3: ç›‘æ§æ€§èƒ½

```python
import time

start = time.time()
output = model(input)
latency = time.time() - start

print(f"Latency: {latency*1000:.2f}ms")
```

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [Tritonæ–‡æ¡£](https://triton-lang.org/)
- [PyTorch C++ Extension](https://pytorch.org/tutorials/advanced/cpp_extension.html)
- [CUDAç¼–ç¨‹æŒ‡å—](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)

### æ¡†æ¶å†…éƒ¨å‚è€ƒ
- `nanovllm/layers/attention.py` - Triton kernelç¤ºä¾‹
- `nanovllm/kernels/triton_ops.py` - è‡ªå®šä¹‰kernelå®ç°
- `nanovllm/layers/custom_mlp.py` - æ¨¡å‹é›†æˆç¤ºä¾‹

## ğŸ’¡ æœ€ä½³å®è·µ

1. âœ… æ€»æ˜¯å…ˆbenchmarkï¼Œç¡®ä¿æœ‰å®é™…åŠ é€Ÿ
2. âœ… æ·»åŠ å®Œå–„çš„é”™è¯¯å¤„ç†å’Œfallback
3. âœ… ç¼–å†™å•å…ƒæµ‹è¯•éªŒè¯æ­£ç¡®æ€§
4. âœ… ä½¿ç”¨profileræ‰¾åˆ°çœŸæ­£çš„ç“¶é¢ˆ
5. âœ… ä¼˜å…ˆä¼˜åŒ–çƒ­ç‚¹è·¯å¾„ï¼ˆforward passä¸­é¢‘ç¹è°ƒç”¨çš„ç®—å­ï¼‰
6. âœ… è€ƒè™‘ä¸åŒçš„batch sizeå’Œåºåˆ—é•¿åº¦

## ğŸ” æ€§èƒ½æ£€æŸ¥æ¸…å•

- [ ] Kernelè¾“å‡ºæ­£ç¡®æ€§å·²éªŒè¯
- [ ] å¯¹æ¯”PyTorchå®ç°æœ‰æ˜æ˜¾åŠ é€Ÿï¼ˆ>1.2xï¼‰
- [ ] åœ¨ä¸åŒè¾“å…¥å°ºå¯¸ä¸‹éƒ½è¡¨ç°è‰¯å¥½
- [ ] å†…å­˜ä½¿ç”¨åˆç†
- [ ] æ·»åŠ äº†fallbackå®ç°
- [ ] å·²ä½¿ç”¨profileråˆ†æ
- [ ] ä»£ç æœ‰é€‚å½“çš„æ³¨é‡Šå’Œæ–‡æ¡£

## ğŸš¦ ä¸‹ä¸€æ­¥

1. **å­¦ä¹ **: é˜…è¯»`CUSTOM_CUDA_KERNEL_GUIDE.md`äº†è§£è¯¦ç»†å®ç°
2. **å®è·µ**: è¿è¡Œ`test_custom_kernels.py`æŸ¥çœ‹ç¤ºä¾‹
3. **å¼€å‘**: åŸºäºæ¨¡æ¿åˆ›å»ºä½ è‡ªå·±çš„kernel
4. **é›†æˆ**: å°†ä¼˜åŒ–åçš„kernelé›†æˆåˆ°æ¨¡å‹ä¸­
5. **æµ‹è¯•**: åœ¨å®é™…workloadä¸Šbenchmarkæ€§èƒ½

---

**é—®é¢˜æˆ–å»ºè®®ï¼Ÿ** 

æŸ¥çœ‹é¡¹ç›®Issuesæˆ–æäº¤æ–°çš„Issueã€‚Happy optimizing! ğŸš€
