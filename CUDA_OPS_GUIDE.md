# CUDA C++ ç®—å­å¼€å‘å®Œæ•´æŒ‡å—

æœ¬æŒ‡å—æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„ CUDA C++ ç®—å­å¼€å‘ç¤ºä¾‹ï¼Œä»ç¼–å†™åˆ°æµ‹è¯•çš„å…¨æµç¨‹ã€‚

## ğŸ“¦ ç¤ºä¾‹å†…å®¹

æœ¬é¡¹ç›®åŒ…å«ä¸€ä¸ªå®Œæ•´çš„ **Fused SiLU + Multiply** CUDA ç®—å­å®ç°ï¼š

```
nanovllm/kernels/cuda_ops/
â”œâ”€â”€ fused_silu_mul.cu      # CUDA C++ kernelå®ç° (æ ¸å¿ƒ)
â”œâ”€â”€ setup.py               # ç¼–è¯‘é…ç½®
â”œâ”€â”€ __init__.py            # Pythonæ¥å£å°è£…
â”œâ”€â”€ test_cuda_ops.py       # å®Œæ•´æµ‹è¯•å¥—ä»¶
â””â”€â”€ README.md              # è¯¦ç»†æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿç¼–è¯‘å’Œæµ‹è¯•

### æ–¹æ³•1: JITç¼–è¯‘ï¼ˆæ¨èå¼€å‘æ—¶ä½¿ç”¨ï¼‰

```bash
# åœ¨è¿œç¨‹æœºå™¨ä¸Š
ssh chenlinfeng@100.99.110.183
source /data/nano/bin/activate
cd /data/python/vllm/github/nano-vllm

# ç›´æ¥è¿è¡Œæµ‹è¯•ï¼ˆä¼šè‡ªåŠ¨JITç¼–è¯‘ï¼‰
python nanovllm/kernels/cuda_ops/test_cuda_ops.py
```

### æ–¹æ³•2: é¢„ç¼–è¯‘ï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰

```bash
# è¿›å…¥CUDAç®—å­ç›®å½•
cd nanovllm/kernels/cuda_ops

# ç¼–è¯‘å®‰è£…
python setup.py install

# æˆ–å¼€å‘æ¨¡å¼ï¼ˆä¿®æ”¹ä»£ç ä¼šç«‹å³ç”Ÿæ•ˆï¼‰
python setup.py develop

# æµ‹è¯•
python test_cuda_ops.py
```

## ğŸ“– æ ¸å¿ƒä»£ç è§£æ

### 1. CUDA Kernelå®ç° (fused_silu_mul.cu)

```cuda
// è®¾å¤‡å‡½æ•°ï¼šSiLUæ¿€æ´»
__device__ __forceinline__ float silu(float x) {
    return x / (1.0f + expf(-x));
}

// åŸºç¡€kernel
__global__ void fused_silu_mul_kernel(
    const float* __restrict__ gate,
    const float* __restrict__ up,
    float* __restrict__ output,
    int n_elements) {
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < n_elements) {
        // èåˆï¼šsilu(gate) * up
        float gate_val = gate[idx];
        float up_val = up[idx];
        float silu_val = silu(gate_val);
        output[idx] = silu_val * up_val;
    }
}
```

**å…³é”®ä¼˜åŒ–ç‚¹**ï¼š
- `__restrict__`: å‘Šè¯‰ç¼–è¯‘å™¨æŒ‡é’ˆä¸ä¼šé‡å ï¼Œå…è®¸æ›´æ¿€è¿›çš„ä¼˜åŒ–
- `__forceinline__`: å¼ºåˆ¶å†…è”è®¾å¤‡å‡½æ•°
- å‘é‡åŒ–ç‰ˆæœ¬ (float4): ä¸€æ¬¡å¤„ç†4ä¸ªå…ƒç´ 

### 2. C++ Wrapperå‡½æ•°

```cuda
torch::Tensor fused_silu_mul_forward(
    torch::Tensor gate,
    torch::Tensor up) {
    
    // å‚æ•°æ£€æŸ¥
    TORCH_CHECK(gate.is_cuda(), "gate must be CUDA tensor");
    TORCH_CHECK(up.is_cuda(), "up must be CUDA tensor");
    
    auto output = torch::empty_like(gate);
    int n_elements = gate.numel();
    
    // å¯åŠ¨kernel
    const int threads = 256;
    const int blocks = (n_elements + threads - 1) / threads;
    
    fused_silu_mul_kernel<<<blocks, threads>>>(
        gate.data_ptr<float>(),
        up.data_ptr<float>(),
        output.data_ptr<float>(),
        n_elements
    );
    
    return output;
}
```

### 3. PyBind11ç»‘å®š

```cuda
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("fused_silu_mul_forward", 
          &fused_silu_mul_forward, 
          "Fused SiLU + Multiply forward (CUDA)");
}
```

### 4. Pythonæ¥å£å°è£… (__init__.py)

```python
def fused_silu_mul(gate: torch.Tensor, up: torch.Tensor) -> torch.Tensor:
    """Fused SiLU + Multiply"""
    if CUDA_OPS_AVAILABLE and gate.is_cuda:
        return _cuda_ops.fused_silu_mul_forward(gate, up)
    else:
        # Fallback
        return torch.nn.functional.silu(gate) * up
```

## ğŸ¯ å®é™…åº”ç”¨ç¤ºä¾‹

### åœ¨MLPå±‚ä¸­ä½¿ç”¨

```python
import torch
from torch import nn
from nanovllm.kernels.cuda_ops import fused_silu_mul

class OptimizedGatedMLP(nn.Module):
    """ä½¿ç”¨CUDAä¼˜åŒ–çš„Gated MLP"""
    
    def __init__(self, hidden_size=4096, intermediate_size=11008):
        super().__init__()
        # ä½¿ç”¨merged projectionå‡å°‘å†…å­˜è®¿é—®
        self.gate_up_proj = nn.Linear(hidden_size, 2 * intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.intermediate_size = intermediate_size
    
    def forward(self, x):
        # ä¸€æ¬¡çŸ©é˜µä¹˜æ³•è·å¾—gateå’Œup
        gate_up = self.gate_up_proj(x)
        gate, up = gate_up.split(self.intermediate_size, dim=-1)
        
        # ä½¿ç”¨CUDAä¼˜åŒ–çš„fused kernel
        # æ›¿ä»£: intermediate = F.silu(gate) * up
        intermediate = fused_silu_mul(gate, up)
        
        return self.down_proj(intermediate)
```

### Benchmarkå¯¹æ¯”

```python
import torch
import torch.nn.functional as F
import time

# æµ‹è¯•æ•°æ®
gate = torch.randn(16, 128, 4096, device='cuda')
up = torch.randn(16, 128, 4096, device='cuda')

# CUDAç‰ˆæœ¬
start = time.time()
for _ in range(100):
    output_cuda = fused_silu_mul(gate, up)
torch.cuda.synchronize()
time_cuda = time.time() - start

# PyTorchç‰ˆæœ¬
start = time.time()
for _ in range(100):
    output_torch = F.silu(gate) * up
torch.cuda.synchronize()
time_torch = time.time() - start

print(f"CUDA: {time_cuda*10:.3f}ms")
print(f"PyTorch: {time_torch*10:.3f}ms")
print(f"Speedup: {time_torch/time_cuda:.2f}x")
```

## ğŸ”§ ç¼–è¯‘å‚æ•°è¯¦è§£

### NVCCç¼–è¯‘é€‰é¡¹ (setup.py)

```python
extra_compile_args={
    'nvcc': [
        '-O3',                          # æœ€é«˜ä¼˜åŒ–ç­‰çº§
        '--use_fast_math',              # ä½¿ç”¨å¿«é€Ÿæ•°å­¦åº“
        '-std=c++14',                   # C++14æ ‡å‡†
        
        # GPUæ¶æ„æ”¯æŒ
        '-gencode=arch=compute_70,code=sm_70',  # V100
        '-gencode=arch=compute_80,code=sm_80',  # A100
        '-gencode=arch=compute_86,code=sm_86',  # RTX 3090
        '-gencode=arch=compute_89,code=sm_89',  # RTX 4090
        
        # é«˜çº§ç‰¹æ€§
        '--expt-relaxed-constexpr',     # æ”¾æ¾constexpré™åˆ¶
        '--expt-extended-lambda',       # æ‰©å±•lambdaæ”¯æŒ
    ]
}
```

## ğŸ“Š æ€§èƒ½åˆ†æ

### æœŸæœ›æ€§èƒ½æå‡

åœ¨å…¸å‹åœºæ™¯ä¸‹ï¼ˆA100 GPU, batch=16, seq_len=128, hidden=4096ï¼‰ï¼š

| æŒ‡æ ‡ | PyTorch | CUDA Fused | æ”¹è¿› |
|------|---------|------------|------|
| å»¶è¿Ÿ | ~0.15ms | ~0.10ms | 1.5x faster |
| å†…å­˜ | 3 æ¬¡è®¿é—® | 1 æ¬¡è®¿é—® | 67% reduction |
| å¸¦å®½åˆ©ç”¨ç‡ | 60% | 90% | 50% improvement |

### ä¸ºä»€ä¹ˆèƒ½åŠ é€Ÿï¼Ÿ

1. **å‡å°‘kernelå¯åŠ¨æ¬¡æ•°**: 2æ¬¡ â†’ 1æ¬¡
2. **å‡å°‘å†…å­˜è®¿é—®**: 
   - PyTorch: load gate â†’ store silu(gate) â†’ load silu(gate) â†’ load up â†’ store result
   - CUDA: load gate â†’ load up â†’ store result
3. **æé«˜ç¼“å­˜å‘½ä¸­ç‡**: æ•°æ®åœ¨å¯„å­˜å™¨ä¸­ç›´æ¥å¤„ç†

## ğŸ› å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### é—®é¢˜1: ç¼–è¯‘é”™è¯¯ "cannot find -lcuda"

**è§£å†³**:
```bash
# æ£€æŸ¥CUDAè·¯å¾„
which nvcc
echo $CUDA_HOME

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
```

### é—®é¢˜2: JITç¼–è¯‘å¾ˆæ…¢

**è§£å†³**: ä½¿ç”¨é¢„ç¼–è¯‘
```bash
python setup.py install
```

### é—®é¢˜3: è¿è¡Œæ—¶é”™è¯¯ "CUDA error: invalid device function"

**åŸå› **: GPUæ¶æ„ä¸åŒ¹é…

**è§£å†³**: åœ¨setup.pyä¸­æ·»åŠ ä½ çš„GPUæ¶æ„
```python
'-gencode=arch=compute_XX,code=sm_XX'  # æ›¿æ¢XXä¸ºä½ çš„æ¶æ„
```

æŸ¥æ‰¾GPUæ¶æ„ï¼š
```bash
nvidia-smi --query-gpu=compute_cap --format=csv
```

## ğŸ“ è¿›é˜¶ä¼˜åŒ–æŠ€å·§

### 1. Shared Memoryä¼˜åŒ–

```cuda
__global__ void optimized_kernel(...) {
    __shared__ float shared_data[BLOCK_SIZE];
    
    // ä½¿ç”¨å…±äº«å†…å­˜å‡å°‘å…¨å±€å†…å­˜è®¿é—®
    shared_data[threadIdx.x] = global_data[idx];
    __syncthreads();
    
    // å¤„ç†...
}
```

### 2. Warp-levelä¼˜åŒ–

```cuda
#include <cuda_fp16.h>

__global__ void warp_optimized(...) {
    // ä½¿ç”¨warp shuffleå‡å°‘shared memoryä½¿ç”¨
    float val = data[idx];
    val = __shfl_xor_sync(0xffffffff, val, 1);
}
```

### 3. Tensor CoreåŠ é€Ÿ (FP16)

```cuda
#include <mma.h>
using namespace nvcuda;

// ä½¿ç”¨Tensor Coreè¿›è¡ŒçŸ©é˜µä¹˜æ³•
wmma::fragment<...> a, b, c;
wmma::load_matrix_sync(a, ...);
wmma::load_matrix_sync(b, ...);
wmma::mma_sync(c, a, b, c);
```

## ğŸ“š å­¦ä¹ èµ„æº

### æ¨èé˜…è¯»
1. [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
2. [CUDA Best Practices](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)
3. [PyTorch C++ Extension Tutorial](https://pytorch.org/tutorials/advanced/cpp_extension.html)

### ç›¸å…³å·¥å…·
- **Nsight Compute**: CUDA kernelæ€§èƒ½åˆ†æ
- **cuda-gdb**: CUDAè°ƒè¯•å™¨
- **nvprof**: CUDA profiler

## ğŸ¯ ä¸‹ä¸€æ­¥

1. **è¿è¡Œæµ‹è¯•**: `python nanovllm/kernels/cuda_ops/test_cuda_ops.py`
2. **æŸ¥çœ‹æ€§èƒ½**: å¯¹æ¯”CUDAå’ŒPyTorchå®ç°
3. **é›†æˆåˆ°æ¨¡å‹**: åœ¨å®é™…MLPå±‚ä¸­ä½¿ç”¨
4. **å¼€å‘æ–°ç®—å­**: å‚è€ƒè¿™ä¸ªç¤ºä¾‹å®ç°å…¶ä»–fusionç®—å­

## ğŸ“ æ€»ç»“

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†ï¼š
- âœ… å®Œæ•´çš„CUDA C++ç®—å­å¼€å‘æµç¨‹
- âœ… ä»kernelå®ç°åˆ°Pythonæ¥å£çš„å®Œæ•´é“¾è·¯
- âœ… æ€§èƒ½ä¼˜åŒ–æŠ€å·§ï¼ˆå‘é‡åŒ–ã€FP16æ”¯æŒï¼‰
- âœ… å®Œå–„çš„æµ‹è¯•å’Œæ–‡æ¡£

ä½ ç°åœ¨å¯ä»¥åŸºäºè¿™ä¸ªæ¨¡æ¿å¼€å‘è‡ªå·±çš„CUDAç®—å­äº†ï¼

---

**ä½œè€…**: nano-vllmå›¢é˜Ÿ  
**æ›´æ–°æ—¶é—´**: 2025-11-27
