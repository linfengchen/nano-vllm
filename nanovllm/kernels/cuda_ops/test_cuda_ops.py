"""
CUDAç®—å­æµ‹è¯•è„šæœ¬

æµ‹è¯•å†…å®¹ï¼š
1. æ­£ç¡®æ€§éªŒè¯
2. æ€§èƒ½Benchmark
3. ä¸åŒæ•°æ®ç±»å‹æµ‹è¯•
4. å†…å­˜ä½¿ç”¨åˆ†æ
"""

import torch
import torch.nn.functional as F
import time
import sys
import os

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))

from nanovllm.kernels.cuda_ops import fused_silu_mul, CUDA_OPS_AVAILABLE


def test_correctness():
    """æµ‹è¯•æ­£ç¡®æ€§"""
    print("=" * 70)
    print("æµ‹è¯•1: æ­£ç¡®æ€§éªŒè¯")
    print("=" * 70)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    # æµ‹è¯•ä¸åŒå°ºå¯¸
    test_shapes = [
        (16, 128, 4096),      # å°batch
        (32, 512, 11008),     # ä¸­ç­‰batch
        (4, 2048, 4096),      # é•¿åºåˆ—
    ]
    
    for shape in test_shapes:
        print(f"\næµ‹è¯•shape: {shape}")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        gate = torch.randn(shape, device='cuda', dtype=torch.float32)
        up = torch.randn(shape, device='cuda', dtype=torch.float32)
        
        # CUDA kernelç»“æœ
        output_cuda = fused_silu_mul(gate, up)
        
        # PyTorchå‚è€ƒå®ç°
        output_torch = F.silu(gate) * up
        
        # æ¯”è¾ƒç»“æœ
        max_diff = (output_cuda - output_torch).abs().max().item()
        mean_diff = (output_cuda - output_torch).abs().mean().item()
        
        print(f"  æœ€å¤§å·®å¼‚: {max_diff:.2e}")
        print(f"  å¹³å‡å·®å¼‚: {mean_diff:.2e}")
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å¯æ¥å—èŒƒå›´å†…
        if max_diff < 1e-5:
            print(f"  âœ… é€šè¿‡")
        else:
            print(f"  âŒ å¤±è´¥ (å·®å¼‚è¿‡å¤§)")
    
    print("\n" + "=" * 70)


def test_fp16():
    """æµ‹è¯•FP16æ”¯æŒ"""
    print("\næµ‹è¯•2: FP16æ•°æ®ç±»å‹")
    print("=" * 70)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    shape = (16, 128, 4096)
    gate = torch.randn(shape, device='cuda', dtype=torch.float16)
    up = torch.randn(shape, device='cuda', dtype=torch.float16)
    
    try:
        output_cuda = fused_silu_mul(gate, up)
        output_torch = F.silu(gate) * up
        
        max_diff = (output_cuda.float() - output_torch.float()).abs().max().item()
        print(f"FP16æœ€å¤§å·®å¼‚: {max_diff:.2e}")
        
        if max_diff < 1e-3:  # FP16ç²¾åº¦è¾ƒä½
            print("âœ… FP16æµ‹è¯•é€šè¿‡")
        else:
            print(f"âŒ FP16æµ‹è¯•å¤±è´¥")
    except Exception as e:
        print(f"âŒ FP16æµ‹è¯•å‡ºé”™: {e}")
    
    print("=" * 70)


def benchmark():
    """æ€§èƒ½Benchmark"""
    print("\næµ‹è¯•3: æ€§èƒ½Benchmark")
    print("=" * 70)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    if not CUDA_OPS_AVAILABLE:
        print("âš ï¸  CUDAç®—å­æœªç¼–è¯‘ï¼Œä»…æµ‹è¯•PyTorchå®ç°")
    
    # æµ‹è¯•é…ç½®
    shape = (16, 128, 4096)
    warmup = 20
    iters = 100
    
    gate = torch.randn(shape, device='cuda', dtype=torch.float32)
    up = torch.randn(shape, device='cuda', dtype=torch.float32)
    
    # Warmup
    for _ in range(warmup):
        if CUDA_OPS_AVAILABLE:
            _ = fused_silu_mul(gate, up)
        _ = F.silu(gate) * up
    
    # Benchmark CUDA kernel
    if CUDA_OPS_AVAILABLE:
        torch.cuda.synchronize()
        start = time.time()
        for _ in range(iters):
            output = fused_silu_mul(gate, up)
        torch.cuda.synchronize()
        time_cuda = (time.time() - start) / iters
        print(f"CUDA Kernel: {time_cuda * 1000:.3f} ms")
    
    # Benchmark PyTorch
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(iters):
        output = F.silu(gate) * up
    torch.cuda.synchronize()
    time_torch = (time.time() - start) / iters
    print(f"PyTorch:     {time_torch * 1000:.3f} ms")
    
    if CUDA_OPS_AVAILABLE:
        speedup = time_torch / time_cuda
        print(f"\nåŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        if speedup > 1.0:
            print(f"âœ… CUDA kernelæ›´å¿« ({speedup:.2f}x)")
        else:
            print(f"âš ï¸  CUDA kernelè¾ƒæ…¢ ({1/speedup:.2f}x slower)")
    
    print("=" * 70)


def benchmark_different_sizes():
    """ä¸åŒå°ºå¯¸çš„æ€§èƒ½æµ‹è¯•"""
    print("\næµ‹è¯•4: ä¸åŒå°ºå¯¸æ€§èƒ½åˆ†æ")
    print("=" * 70)
    
    if not torch.cuda.is_available() or not CUDA_OPS_AVAILABLE:
        print("âŒ è·³è¿‡æµ‹è¯•")
        return
    
    test_configs = [
        ("å°batch, çŸ­åºåˆ—", (4, 64, 4096)),
        ("ä¸­batch, ä¸­åºåˆ—", (16, 128, 4096)),
        ("å¤§batch, é•¿åºåˆ—", (32, 512, 4096)),
        ("è¶…å¤§hidden", (16, 128, 11008)),
    ]
    
    print(f"\n{'é…ç½®':<20} {'CUDA(ms)':<12} {'PyTorch(ms)':<12} {'åŠ é€Ÿæ¯”':<10}")
    print("-" * 70)
    
    for name, shape in test_configs:
        gate = torch.randn(shape, device='cuda')
        up = torch.randn(shape, device='cuda')
        
        warmup = 10
        iters = 50
        
        # Warmup
        for _ in range(warmup):
            _ = fused_silu_mul(gate, up)
            _ = F.silu(gate) * up
        
        # CUDA
        torch.cuda.synchronize()
        start = time.time()
        for _ in range(iters):
            _ = fused_silu_mul(gate, up)
        torch.cuda.synchronize()
        time_cuda = (time.time() - start) / iters * 1000
        
        # PyTorch
        torch.cuda.synchronize()
        start = time.time()
        for _ in range(iters):
            _ = F.silu(gate) * up
        torch.cuda.synchronize()
        time_torch = (time.time() - start) / iters * 1000
        
        speedup = time_torch / time_cuda
        print(f"{name:<20} {time_cuda:>10.3f}  {time_torch:>10.3f}  {speedup:>8.2f}x")
    
    print("=" * 70)


def test_memory():
    """å†…å­˜ä½¿ç”¨æµ‹è¯•"""
    print("\næµ‹è¯•5: å†…å­˜ä½¿ç”¨åˆ†æ")
    print("=" * 70)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    shape = (16, 128, 4096)
    
    # æµ‹è¯•CUDA kernel
    if CUDA_OPS_AVAILABLE:
        torch.cuda.reset_peak_memory_stats()
        gate = torch.randn(shape, device='cuda')
        up = torch.randn(shape, device='cuda')
        output = fused_silu_mul(gate, up)
        mem_cuda = torch.cuda.max_memory_allocated() / 1024**2
        print(f"CUDA Kernelå³°å€¼å†…å­˜: {mem_cuda:.2f} MB")
        del gate, up, output
    
    # æµ‹è¯•PyTorch
    torch.cuda.reset_peak_memory_stats()
    gate = torch.randn(shape, device='cuda')
    up = torch.randn(shape, device='cuda')
    output = F.silu(gate) * up
    mem_torch = torch.cuda.max_memory_allocated() / 1024**2
    print(f"PyTorchå³°å€¼å†…å­˜:     {mem_torch:.2f} MB")
    
    if CUDA_OPS_AVAILABLE:
        print(f"\nå†…å­˜èŠ‚çœ: {mem_torch - mem_cuda:.2f} MB")
    
    print("=" * 70)


def main():
    print("\n" + "ğŸ”¥" * 35)
    print("CUDAç®—å­å®Œæ•´æµ‹è¯•å¥—ä»¶")
    print("ğŸ”¥" * 35)
    
    if not torch.cuda.is_available():
        print("\nâŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•")
        return
    
    print(f"\nCUDAè®¾å¤‡: {torch.cuda.get_device_name(0)}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAç®—å­çŠ¶æ€: {'âœ… å·²åŠ è½½' if CUDA_OPS_AVAILABLE else 'âŒ æœªåŠ è½½'}")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_correctness()
    test_fp16()
    benchmark()
    benchmark_different_sizes()
    test_memory()
    
    print("\n" + "ğŸ‰" * 35)
    print("æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("ğŸ‰" * 35 + "\n")


if __name__ == "__main__":
    main()
