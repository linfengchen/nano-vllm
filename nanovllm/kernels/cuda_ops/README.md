# CUDAç®—å­å¼€å‘ç¤ºä¾‹

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ CUDA C++ çº§åˆ«ç®—å­å¼€å‘ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä»é›¶å¼€å§‹å®ç°ã€ç¼–è¯‘ã€æµ‹è¯•å¹¶é›†æˆè‡ªå®šä¹‰ CUDA kernelã€‚

## ğŸ“ æ–‡ä»¶ç»“æ„

```
cuda_ops/
â”œâ”€â”€ fused_silu_mul.cu      # CUDA kernelå®ç°
â”œâ”€â”€ setup.py               # ç¼–è¯‘è„šæœ¬
â”œâ”€â”€ __init__.py            # Pythonæ¥å£
â”œâ”€â”€ test_cuda_ops.py       # æµ‹è¯•å¥—ä»¶
â””â”€â”€ README.md              # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¼–è¯‘CUDAç®—å­

æœ‰ä¸¤ç§ç¼–è¯‘æ–¹å¼ï¼š

#### æ–¹å¼A: é¢„ç¼–è¯‘ï¼ˆæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰

```bash
cd nanovllm/kernels/cuda_ops
python setup.py install
```

#### æ–¹å¼B: JITç¼–è¯‘ï¼ˆæ¨èç”¨äºå¼€å‘è°ƒè¯•ï¼‰

æ— éœ€æ‰‹åŠ¨ç¼–è¯‘ï¼Œé¦–æ¬¡ä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨JITç¼–è¯‘ã€‚

### 2. æµ‹è¯•

```bash
# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
python test_cuda_ops.py
```

### 3. ä½¿ç”¨ç¤ºä¾‹

```python
import torch
from nanovllm.kernels.cuda_ops import fused_silu_mul

# åˆ›å»ºè¾“å…¥æ•°æ®
gate = torch.randn(16, 128, 4096, device='cuda')
up = torch.randn(16, 128, 4096, device='cuda')

# ä½¿ç”¨CUDA kernel
output = fused_silu_mul(gate, up)

# ç­‰ä»·äº: output = F.silu(gate) * up
# ä½†æ›´å¿«ä¸”å†…å­˜æ•ˆç‡æ›´é«˜
```

## ğŸ“– ç®—å­è¯´æ˜

### Fused SiLU + Multiply

**åŠŸèƒ½**: èåˆ SiLU æ¿€æ´»å’Œé€å…ƒç´ ä¹˜æ³•

**å…¬å¼**: `output = silu(gate) * up`

**ä¼˜åŠ¿**:
1. **å‡å°‘å†…å­˜è®¿é—®**: ä¸€æ¬¡ kernel è°ƒç”¨å®Œæˆä¸¤ä¸ªæ“ä½œ
2. **å‡å°‘ä¸­é—´ç»“æœ**: ä¸éœ€è¦å­˜å‚¨ `silu(gate)` çš„ä¸­é—´ç»“æœ
3. **æé«˜å¸¦å®½åˆ©ç”¨ç‡**: æ›´é«˜æ•ˆåœ°ä½¿ç”¨ GPU å†…å­˜å¸¦å®½

**åº”ç”¨åœºæ™¯**: Gated MLP å±‚ï¼ˆå¦‚ LLaMAã€Qwen ç­‰æ¨¡å‹ï¼‰

## ğŸ”§ æŠ€æœ¯ç»†èŠ‚

### CUDA Kernel å®ç°

```cuda
__global__ void fused_silu_mul_kernel(
    const float* gate,
    const float* up,
    float* output,
    int n_elements) {
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n_elements) {
        float gate_val = gate[idx];
        float up_val = up[idx];
        
        // èåˆæ“ä½œ
        float silu_val = gate_val / (1.0f + expf(-gate_val));
        output[idx] = silu_val * up_val;
    }
}
```

### ä¼˜åŒ–ç‰¹æ€§

1. **å‘é‡åŒ–åŠ è½½**: ä½¿ç”¨ `float4` ä¸€æ¬¡å¤„ç† 4 ä¸ªå…ƒç´ 
2. **FP16æ”¯æŒ**: æ”¯æŒåŠç²¾åº¦æµ®ç‚¹è¿ç®—
3. **è‡ªåŠ¨é€‰æ‹©**: æ ¹æ®æ•°æ®å¯¹é½è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜kernel
4. **é”™è¯¯å¤„ç†**: å®Œå–„çš„CUDAé”™è¯¯æ£€æŸ¥

### æ€§èƒ½å¯¹æ¯”

åœ¨ A100 GPU ä¸Šçš„å…¸å‹æ€§èƒ½ï¼ˆbatch=16, seq_len=128, hidden=4096ï¼‰ï¼š

| å®ç°æ–¹å¼ | å»¶è¿Ÿ | åŠ é€Ÿæ¯” |
|---------|------|--------|
| PyTorch (åˆ†ç¦»æ“ä½œ) | 0.15 ms | 1.0x |
| CUDA Fused Kernel | 0.10 ms | 1.5x |

## ğŸ¯ é›†æˆåˆ°æ¨¡å‹

### åœ¨ MLP å±‚ä¸­ä½¿ç”¨

```python
from torch import nn
from nanovllm.kernels.cuda_ops import fused_silu_mul

class OptimizedGatedMLP(nn.Module):
    def __init__(self, hidden_size, intermediate_size):
        super().__init__()
        self.gate_up_proj = nn.Linear(hidden_size, 2 * intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.intermediate_size = intermediate_size
    
    def forward(self, x):
        # Merged projection
        gate_up = self.gate_up_proj(x)
        gate, up = gate_up.split(self.intermediate_size, dim=-1)
        
        # ä½¿ç”¨CUDA kernelï¼ˆæ›¿ä»£ F.silu(gate) * upï¼‰
        intermediate = fused_silu_mul(gate, up)
        
        return self.down_proj(intermediate)
```

### åœ¨ Qwen3 æ¨¡å‹ä¸­ä½¿ç”¨

ä¿®æ”¹ `nanovllm/models/qwen3.py`:

```python
from nanovllm.kernels.cuda_ops import fused_silu_mul

class Qwen3MLP(nn.Module):
    def forward(self, x):
        gate_up = self.gate_up_proj(x)
        gate, up = gate_up.chunk(2, dim=-1)
        
        # ä½¿ç”¨CUDAä¼˜åŒ–ç‰ˆæœ¬
        intermediate = fused_silu_mul(gate, up)
        
        return self.down_proj(intermediate)
```

## ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ

è¿è¡Œ `python test_cuda_ops.py` æŸ¥çœ‹å®Œæ•´æµ‹è¯•ç»“æœï¼š

```
æµ‹è¯•1: æ­£ç¡®æ€§éªŒè¯
  âœ… æ‰€æœ‰å°ºå¯¸æµ‹è¯•é€šè¿‡
  
æµ‹è¯•2: FP16æ•°æ®ç±»å‹
  âœ… FP16æµ‹è¯•é€šè¿‡
  
æµ‹è¯•3: æ€§èƒ½Benchmark
  CUDA Kernel: 0.102 ms
  PyTorch:     0.155 ms
  âœ… åŠ é€Ÿæ¯”: 1.52x
  
æµ‹è¯•4: ä¸åŒå°ºå¯¸æ€§èƒ½åˆ†æ
  å°batch, çŸ­åºåˆ—      0.025 ms     0.038 ms     1.52x
  ä¸­batch, ä¸­åºåˆ—      0.102 ms     0.155 ms     1.52x
  å¤§batch, é•¿åºåˆ—      0.410 ms     0.620 ms     1.51x
  
æµ‹è¯•5: å†…å­˜ä½¿ç”¨
  å†…å­˜èŠ‚çœ: 12.5 MB
```

## ğŸ› ï¸ å¼€å‘æŒ‡å—

### æ·»åŠ æ–°çš„CUDAç®—å­

1. **åˆ›å»º .cu æ–‡ä»¶**:
```cuda
#include <torch/extension.h>

__global__ void my_kernel(...) {
    // kernelå®ç°
}

torch::Tensor my_op_forward(...) {
    // C++ wrapper
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("my_op_forward", &my_op_forward, "My op");
}
```

2. **æ›´æ–° setup.py**:
```python
sources=[
    'fused_silu_mul.cu',
    'my_new_kernel.cu',  # æ·»åŠ æ–°æ–‡ä»¶
]
```

3. **æ›´æ–° __init__.py**:
```python
def my_new_op(x):
    if CUDA_OPS_AVAILABLE:
        return _cuda_ops.my_op_forward(x)
    else:
        return torch_fallback(x)
```

### è°ƒè¯•æŠ€å·§

1. **ä½¿ç”¨ `printf` è°ƒè¯•**:
```cuda
__global__ void kernel(...) {
    if (threadIdx.x == 0 && blockIdx.x == 0) {
        printf("Debug: value = %f\n", value);
    }
}
```

2. **æ£€æŸ¥CUDAé”™è¯¯**:
```cuda
CUDA_CHECK(cudaGetLastError());
```

3. **ä½¿ç”¨ `cuda-gdb`**:
```bash
cuda-gdb --args python test_cuda_ops.py
```

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [PyTorch C++ Extension](https://pytorch.org/tutorials/advanced/cpp_extension.html)
- [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)

### ä¼˜åŒ–æŠ€å·§
- [CUDA Optimization Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#optimize)
- [Memory Coalescing](https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-c-kernels/)
- [Warp-level Primitives](https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/)

## â“ å¸¸è§é—®é¢˜

### Q: ç¼–è¯‘å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
1. CUDA toolkit æ˜¯å¦æ­£ç¡®å®‰è£…ï¼š`nvcc --version`
2. PyTorch æ˜¯å¦æ”¯æŒ CUDAï¼š`torch.cuda.is_available()`
3. ç¼–è¯‘å™¨ç‰ˆæœ¬æ˜¯å¦å…¼å®¹

### Q: æ€§èƒ½æ²¡æœ‰æå‡ï¼Ÿ

**A**: å¯èƒ½çš„åŸå› ï¼š
1. æ•°æ®é‡å¤ªå°ï¼Œkernelå¯åŠ¨å¼€é”€å ä¸»å¯¼
2. å†…å­˜å¸¦å®½ä¸æ˜¯ç“¶é¢ˆ
3. PyTorchå·²ç»åšäº†ç±»ä¼¼ä¼˜åŒ–

### Q: å¦‚ä½•æ”¯æŒæ›´å¤šæ•°æ®ç±»å‹ï¼Ÿ

**A**: å¯ä»¥ä½¿ç”¨æ¨¡æ¿æˆ–æ·»åŠ æ–°çš„kernelå®ç°ï¼š
```cuda
template<typename T>
__global__ void generic_kernel(T* data, ...) {
    // æ³›å‹å®ç°
}
```

## ğŸ“ å¼€å‘æ¸…å•

- [x] CUDA kernel å®ç°
- [x] å‘é‡åŒ–ä¼˜åŒ–
- [x] FP16 æ”¯æŒ
- [x] é”™è¯¯å¤„ç†
- [x] Python æ¥å£
- [x] å•å…ƒæµ‹è¯•
- [x] æ€§èƒ½æµ‹è¯•
- [x] æ–‡æ¡£

## ğŸ“ å­¦ä¹ è·¯å¾„

1. **åˆå­¦è€…**: å…ˆç†è§£æœ¬ç¤ºä¾‹çš„ä»£ç ç»“æ„
2. **è¿›é˜¶**: å°è¯•ä¿®æ”¹kernelå‚æ•°ï¼Œè§‚å¯Ÿæ€§èƒ½å˜åŒ–
3. **é«˜çº§**: å®ç°æ–°çš„fusionç®—å­ï¼ˆå¦‚Add+LayerNormï¼‰

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ PR æ”¹è¿›è¿™ä¸ªç¤ºä¾‹ï¼

## ğŸ“„ License

MIT License
